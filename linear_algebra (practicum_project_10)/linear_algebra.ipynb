{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V3</b>\n",
    "\t  \n",
    "Thank you for taking the time to improve the project! It's accepted now. Good luck on the next sprint! :)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review**\n",
    "\t  \n",
    "Hi, my name is Dmitry and I will be reviewing your project.\n",
    "  \n",
    "You can find my comments in colored markdown cells:\n",
    "  \n",
    "<div class=\"alert alert-success\">\n",
    "  If everything is done successfully.\n",
    "</div>\n",
    "  \n",
    "<div class=\"alert alert-warning\">\n",
    "  If I have some (optional) suggestions, or questions to think about, or general comments.\n",
    "</div>\n",
    "  \n",
    "<div class=\"alert alert-danger\">\n",
    "  If a section requires some corrections. Work can't be accepted with red comments.\n",
    "</div>\n",
    "  \n",
    "Please don't remove my comments, as it will make further review iterations much harder for me.\n",
    "  \n",
    "Feel free to reply to my comments or ask questions using the following template:\n",
    "  \n",
    "<div class=\"alert alert-info\">\n",
    "  For your comments and questions.\n",
    "</div>\n",
    "  \n",
    "First of all, thank you for turning in the project! You did a pretty good job overall, but unfortunately there are a couple of problems that need to be fixed before the project can be accepted. Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sure Tomorrow insurance company wants to solve several tasks with the help of Machine Learning and you are asked to evaluate that possibility.\n",
    "\n",
    "- Task 1: Find customers who are similar to a given customer. This will help the company's agents with marketing.\n",
    "- Task 2: Predict whether a new customer is likely to receive an insurance benefit. Can a prediction model do better than a dummy model?\n",
    "- Task 3: Predict the number of insurance benefits a new customer is likely to receive using a linear regression model.\n",
    "- Task 4: Protect clients' personal data without breaking the model from the previous task. It's necessary to develop a data transformation algorithm that would make it hard to recover personal information if the data fell into the wrong hands. This is called data masking, or data obfuscation. But the data should be protected in such a way that the quality of machine learning models doesn't suffer. You don't need to pick the best model, just prove that the algorithm works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Exploration\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.2 MB 129 kB/s eta 0:00:01    |█████████████▊                  | 10.0 MB 1.3 MB/s eta 0:00:10\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn) (1.0.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.0.1 threadpoolctl-3.0.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (20.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from optuna) (1.19.5)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from optuna) (5.4.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.51.0)\n",
      "Collecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.5.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.1)\n",
      "Collecting cliff\n",
      "  Downloading cliff-3.9.0-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 6.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.3)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.3.11)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (1.15.0)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-2.3.0-py3-none-any.whl (24 kB)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 2.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.2.0-py3-none-any.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 23.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting autopage>=0.4.0\n",
      "  Downloading autopage-0.4.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (2.8.1)\n",
      "Requirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.0.4)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.1.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from PrettyTable>=0.7.2->cliff->optuna) (2.0.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from PrettyTable>=0.7.2->cliff->optuna) (0.2.5)\n",
      "Collecting colorama>=0.3.7\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (19.1.0)\n",
      "Collecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->PrettyTable>=0.7.2->cliff->optuna) (3.4.0)\n",
      "Building wheels for collected packages: pyperclip\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11106 sha256=ad5d2637d7938236d1175a89788692d532c94d2ea0895cd3338e5efb024f83a2\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
      "Successfully built pyperclip\n",
      "Installing collected packages: cmaes, colorlog, pbr, PrettyTable, stevedore, colorama, typing-extensions, pyperclip, cmd2, autopage, cliff, optuna\n",
      "\u001b[33m  WARNING: The script pbr is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script optuna is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed PrettyTable-2.3.0 autopage-0.4.0 cliff-3.9.0 cmaes-0.8.2 cmd2-2.2.0 colorama-0.4.4 colorlog-6.5.0 optuna-2.10.0 pbr-5.6.0 pyperclip-1.8.2 stevedore-3.5.0 typing-extensions-3.10.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn --upgrade\n",
    "!pip install optuna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "import sklearn.preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#packages for hyperparameter choice\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and conduct a basic check that it's free from obvious issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/insurance_us.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the colums to make the code look more consistent with its style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Gender': 'gender', 'Age': 'age', 'Salary': 'income', 'Family members': 'family_members', 'Insurance benefits': 'insurance_benefits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>family_members</th>\n",
       "      <th>insurance_benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>46200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3722</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2939</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>21400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>39700.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2185</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2159</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>37300.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2818</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40700.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1654</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>38400.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3964</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>40600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2644</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender   age   income  family_members  insurance_benefits\n",
       "3675       1  35.0  46200.0               1                   0\n",
       "3722       0  21.0  21400.0               0                   0\n",
       "2939       0  37.0  21400.0               0                   0\n",
       "410        0  32.0  39700.0               1                   0\n",
       "2185       0  25.0  58000.0               0                   0\n",
       "2159       0  34.0  37300.0               4                   0\n",
       "2818       0  52.0  40700.0               0                   2\n",
       "1654       0  44.0  38400.0               1                   1\n",
       "3964       1  24.0  40600.0               1                   0\n",
       "2644       1  33.0  40000.0               1                   0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      "gender                5000 non-null int64\n",
      "age                   5000 non-null float64\n",
      "income                5000 non-null float64\n",
      "family_members        5000 non-null int64\n",
      "insurance_benefits    5000 non-null int64\n",
      "dtypes: float64(2), int64(3)\n",
      "memory usage: 195.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may want to fix the age type (from float to int) though this is not critical\n",
    "\n",
    "# write your conversion here if you choose:\n",
    "df['age'] = df['age'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      "gender                5000 non-null int64\n",
      "age                   5000 non-null int64\n",
      "income                5000 non-null float64\n",
      "family_members        5000 non-null int64\n",
      "insurance_benefits    5000 non-null int64\n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 195.4 KB\n"
     ]
    }
   ],
   "source": [
    "# check to see that the conversion was successful\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>family_members</th>\n",
       "      <th>insurance_benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>30.952800</td>\n",
       "      <td>39916.360000</td>\n",
       "      <td>1.194200</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.500049</td>\n",
       "      <td>8.440807</td>\n",
       "      <td>9900.083569</td>\n",
       "      <td>1.091387</td>\n",
       "      <td>0.463183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5300.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>33300.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>40200.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>46600.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>79000.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender          age        income  family_members  \\\n",
       "count  5000.000000  5000.000000   5000.000000     5000.000000   \n",
       "mean      0.499000    30.952800  39916.360000        1.194200   \n",
       "std       0.500049     8.440807   9900.083569        1.091387   \n",
       "min       0.000000    18.000000   5300.000000        0.000000   \n",
       "25%       0.000000    24.000000  33300.000000        0.000000   \n",
       "50%       0.000000    30.000000  40200.000000        1.000000   \n",
       "75%       1.000000    37.000000  46600.000000        2.000000   \n",
       "max       1.000000    65.000000  79000.000000        6.000000   \n",
       "\n",
       "       insurance_benefits  \n",
       "count         5000.000000  \n",
       "mean             0.148000  \n",
       "std              0.463183  \n",
       "min              0.000000  \n",
       "25%              0.000000  \n",
       "50%              0.000000  \n",
       "75%              0.000000  \n",
       "max              5.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now have a look at the data's descriptive statistics. \n",
    "# Does everything look okay?\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly check whether there are certain groups of customers by looking at the pair plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.pairplot(df, kind='hist')\n",
    "g.fig.set_size_inches(12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, it is a bit difficult to spot obvious groups (clusters) as it is difficult to combine several variables simultaneously (to analyze multivariate distributions). That's where LA and ML can be quite handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Similar Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the language of ML, it is necessary to develop a procedure that returns k nearest neighbors (objects) for a given object based on the distance between the objects.\n",
    "\n",
    "You may want to review the following lessons (chapter -> lesson)\n",
    "- Distance Between Vectors -> Euclidean Distance\n",
    "- Distance Between Vectors -> Manhattan Distance\n",
    "\n",
    "To solve the task, we can try different distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that returns k nearest neighbors for an $n^{th}$ object based on a specified distance metric. The number of received insurance benefits should not be taken into account for this task. \n",
    "\n",
    "You can use a ready implementation of the kNN algorithm from scikit-learn (check [the link](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors)) or use your own.\n",
    "\n",
    "Test it for four combination of two cases\n",
    "- Scaling\n",
    "  - the data is not scaled\n",
    "  - the data is scaled with the [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html) scaler\n",
    "- Distance Metrics\n",
    "  - Euclidean\n",
    "  - Manhattan\n",
    "\n",
    "Answer these questions:\n",
    "- Does the data being not scaled affect the kNN algorithm? If so, how does that appear?\n",
    "- How similar are the results using the Manhattan distance metric (regardless of the scaling)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['gender', 'age', 'income', 'family_members']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn(df, n, k, metric):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns k nearest neighbors\n",
    "\n",
    "    :param df: pandas DataFrame used to find similar objects within\n",
    "    :param n: object no for which the nearest neighbours are looked for\n",
    "    :param k: the number of the nearest neighbours to return\n",
    "    :param metric: name of distance metric\n",
    "    \"\"\"\n",
    "\n",
    "    nbrs = sklearn.neighbors.NearestNeighbors(n_neighbors=k,metric=metric).fit(df[feature_names])\n",
    "    nbrs_distances, nbrs_indices = nbrs.kneighbors([df.iloc[n][feature_names]], k, return_distance=True)\n",
    "    \n",
    "    df_res = pd.concat([\n",
    "        df.iloc[nbrs_indices[0]], \n",
    "        pd.DataFrame(nbrs_distances.T, index=nbrs_indices[0], columns=['distance'])\n",
    "        ], axis=1)\n",
    "    \n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\t  \n",
    "The function for getting k nearest neighbors is correct!\n",
    "\t  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['gender', 'age', 'income', 'family_members']\n",
    "\n",
    "transformer_mas = sklearn.preprocessing.MaxAbsScaler().fit(df[feature_names].to_numpy())\n",
    "\n",
    "df_scaled = df.copy()\n",
    "df_scaled.loc[:, feature_names] = transformer_mas.transform(df[feature_names].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>family_members</th>\n",
       "      <th>insurance_benefits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.491139</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.446154</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.449367</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476923</td>\n",
       "      <td>0.455696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4838</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>0.375949</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender       age    income  family_members  insurance_benefits\n",
       "1028     1.0  0.353846  0.491139        0.500000                   0\n",
       "3097     1.0  0.446154  0.670886        0.000000                   0\n",
       "1626     1.0  0.569231  0.449367        0.166667                   0\n",
       "4398     0.0  0.476923  0.455696        0.000000                   0\n",
       "4838     1.0  0.784615  0.375949        0.000000                   2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get similar records for a given one for every combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2c0860057a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf_euclidean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf_scaled_euclidean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf_manhattan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'manhattan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf_scaled_manhattan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'manhattan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_euclidean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_scaled_euclidean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-68ee01f15603>\u001b[0m in \u001b[0;36mget_knn\u001b[0;34m(df, n, k, metric)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mnbrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mnbrs_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbrs_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/neighbors/_unsupervised.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mnearest\u001b[0m \u001b[0mneighbors\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKDTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_algorithm_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdtype_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                 \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mdtype_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Int\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"UInt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m                 \u001b[0;31m# name looks like an Integer Extension Array, now check for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# the dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_get\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# append bit counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_name_includes_bit_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/_dtype.py\u001b[0m in \u001b[0;36m_name_includes_bit_suffix\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# implied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflexible\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_isunsized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# unspecified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \"\"\"\n\u001b[0;32m--> 387\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubclass_\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "euc_seuc = 0\n",
    "euc_man = 0\n",
    "seuc_sman = 0\n",
    "sman_man = 0\n",
    "for n in range(len(df)):\n",
    "    df_euclidean = get_knn(df,n,5,'euclidean')\n",
    "    df_scaled_euclidean = get_knn(df_scaled,n,5,'euclidean')\n",
    "    df_manhattan = get_knn(df,n,5,'manhattan')\n",
    "    df_scaled_manhattan = get_knn(df_scaled,n,5,'manhattan')\n",
    "    if np.array_equal(np.sort(np.array(df_euclidean.index)),np.sort(np.array(df_scaled_euclidean.index))) == True:\n",
    "        euc_seuc += 1\n",
    "    if np.array_equal(np.sort(np.array(df_euclidean.index)),np.sort(np.array(df_manhattan.index))) == True:\n",
    "        euc_man += 1\n",
    "    if np.array_equal(np.sort(np.array(df_scaled_euclidean.index)),np.sort(np.array(df_scaled_manhattan.index)))== True:\n",
    "        seuc_sman += 1\n",
    "    if np.array_equal(np.sort(np.array(df_scaled_manhattan.index)),np.sort(np.array(df_manhattan.index)))== True:\n",
    "        sman_man += 1\n",
    "print(\n",
    "    'Percent of obsersvations with the exact same 5 nearest neighbors for:', '\\n' \n",
    "    'Euclidean and Scaled Euclidean Datasets:', round(euc_seuc/len(df)*100,2), '\\n'\n",
    "    'Euclidean and Manhattan Datasets:',round(euc_man/len(df)*100,2), '\\n'\n",
    "    'Scaled Euclidean and Scaled Manhattan Datasets:',round(seuc_sman/len(df)*100,2), '\\n'\n",
    "    'Manhattan and Scaled Manhattan Datasets:',round(sman_man/len(df)*100,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers to the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does the data being not scaled affect the kNN algorithm? If so, how does that appear?** \n",
    "\n",
    "It clearly has an effect on the algorithm because it is very rare that the scaled and non-scaled datasets return the same 5 nearest neighbors. This is because when you scale the data it will change the distances between observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How similar are the results using the Manhattan distance metric (regardless of the scaling)?** \n",
    "\n",
    "Changing the distance measurement did not have nearly the same effect as scaling the data. As you can see above, changing the distance measurement, in the non-scaled data, only changed 25% of the top 5 nearest neighbors. Even in the scaled data, over 60% of the observations had the same 5 nearest neighbors for both distance measurements. The reason the scaled data has a lower percentage of observations with the same 5 nearest neighbors is due to the fact that all the observations are clumped closer together.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Reviewer's comment</b>\n",
    "\t  \n",
    "Ok! The main difference between kNN on scaled and unscaled data is that on unscaled data if some feature has a much bigger magnitude than others, it will dominate the metric. Due to this reason, distance-based algorithms like kNN are sensitive to scales of features.\n",
    "\t  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Is Customer Likely to Receive Insurance Benefit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of machine learning we can look at this like a binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `insurance_benefits` being more than zero as the target, evaluate whether the kNN classification approach can do better than a dummy model.\n",
    "\n",
    "Instructions:\n",
    "- Build a KNN-based classifier and measure its quality with the F1 metric for k=1..10 for both the original data and the scaled one. That'd be interesting to see how k may influece the evaluation metric, and whether scaling the data makes any difference. You can use a ready implemention of the kNN classification algorithm from scikit-learn (check [the link](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) or use your own.\n",
    "- Build the dummy model which is just random for this case. It should return \"1\" with some probability. Let's test the model with four probability values: 0, the probability of paying any insurance benefit, 0.5, 1.\n",
    "\n",
    "The probability of paying any insurance benefit can be defined as\n",
    "\n",
    "$$\n",
    "P\\{\\text{insurance benefit received}\\}=\\frac{\\text{number of clients received any insurance benefit}}{\\text{total number of clients}}.\n",
    "$$\n",
    "\n",
    "Split the whole data in the 70:30 proportion for the training/testing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the target\n",
    "df['insurance_benefits_received'] = df['insurance_benefits']/df['insurance_benefits']\n",
    "\n",
    "df['insurance_benefits_received'].fillna(0,inplace=True)\n",
    "\n",
    "target = df['insurance_benefits_received']\n",
    "features = df.drop(['insurance_benefits_received','insurance_benefits'],axis=1)\n",
    "\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the target\n",
    "df_scaled['insurance_benefits_received'] = df_scaled['insurance_benefits']/df_scaled['insurance_benefits']\n",
    "\n",
    "df_scaled['insurance_benefits_received'].fillna(0,inplace=True)\n",
    "\n",
    "target_scaled = df_scaled['insurance_benefits_received']\n",
    "features_scaled = df_scaled.drop(['insurance_benefits_received','insurance_benefits'],axis=1)\n",
    "\n",
    "\n",
    "features_train_scaled, features_test_scaled, target_train_scaled, target_test_scaled = train_test_split(features_scaled, target_scaled, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    4436\n",
       "1.0     564\n",
       "Name: insurance_benefits_received, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the class imbalance with value_counts()\n",
    "\n",
    "df['insurance_benefits_received'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classifier(y_true, y_pred):\n",
    "    \n",
    "    f1_score = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(f'F1: {f1_score:.2f}')\n",
    "    \n",
    "# if you have an issue with the following line, restart the kernel and run the notebook again\n",
    "    cm = sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='all')\n",
    "    print('Confusion Matrix')\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating output of a random model\n",
    "\n",
    "def rnd_model_predict(P, size, seed=42):\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    return rng.binomial(n=1, p=P, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability: 0.00\n",
      "F1: 0.00\n",
      "Confusion Matrix\n",
      "[[0.8872 0.    ]\n",
      " [0.1128 0.    ]]\n",
      "\n",
      "The probability: 0.11\n",
      "F1: 0.12\n",
      "Confusion Matrix\n",
      "[[0.7914 0.0958]\n",
      " [0.0994 0.0134]]\n",
      "\n",
      "The probability: 0.50\n",
      "F1: 0.20\n",
      "Confusion Matrix\n",
      "[[0.456  0.4312]\n",
      " [0.053  0.0598]]\n",
      "\n",
      "The probability: 1.00\n",
      "F1: 0.20\n",
      "Confusion Matrix\n",
      "[[0.     0.8872]\n",
      " [0.     0.1128]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for P in [0, df['insurance_benefits_received'].sum() / len(df), 0.5, 1]:\n",
    "\n",
    "    print(f'The probability: {P:.2f}')\n",
    "    y_pred_rnd = rnd_model_predict(P,len(df))\n",
    "        \n",
    "    eval_classifier(df['insurance_benefits_received'], y_pred_rnd)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest F1 score for non-scaled data 0.65\n",
      "{'n_neighbors': 1}\n",
      "Highest F1 score for scaled data 0.9392971246006389\n",
      "{'n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "def KNN_objective(trial):\n",
    "    #defining ranges for decison tree optimization\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 10,1)\n",
    "    #creating model\n",
    "    model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    model.fit(features_train,target_train)\n",
    "    target_pred = model.predict(features_test)\n",
    "    f1_score = sklearn.metrics.f1_score(target_test, target_pred)\n",
    "    #returning the score that compares the predicted validation targets to the actual validation targets\n",
    "    return f1_score\n",
    "#creating the study to optimize the hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize',sampler=TPESampler(seed=15))\n",
    "study_knn.optimize(KNN_objective, n_trials=100)\n",
    "\n",
    "#printing the highest f1_score\n",
    "print('Highest F1 score for non-scaled data',study_knn.best_value)\n",
    "print(study_knn.best_params)\n",
    "\n",
    "def KNN_scaled_objective(trial):\n",
    "    #defining ranges for decison tree optimization\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 10,1)\n",
    "    #creating model\n",
    "    model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    model.fit(features_train_scaled,target_train_scaled)\n",
    "    target_pred_scaled = model.predict(features_test_scaled)\n",
    "    f1_score = sklearn.metrics.f1_score(target_test_scaled, target_pred_scaled)\n",
    "    #returning the score that compares the predicted validation targets to the actual validation targets\n",
    "    return f1_score\n",
    "#creating the study to optimize the hyperparameters\n",
    "study_knn_scaled = optuna.create_study(direction='maximize',sampler=TPESampler(seed=15))\n",
    "study_knn_scaled.optimize(KNN_scaled_objective, n_trials=100)\n",
    "\n",
    "#printing the highest f1_score\n",
    "print('Highest F1 score for scaled data',study_knn_scaled.best_value)\n",
    "print(study_knn_scaled.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results above show that the KNeighborsClassifier model performs best when looking at the three closest neighbors using the scaled data.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<b>Reviewer's comment</b>\n",
    "\t  \n",
    "Ok! The task is not quite complete though. The task requires you to:\n",
    " \n",
    "1. try different value of `n_neighbors` from 1 to 10, but it looks like you only tried one value (the default one)\n",
    "2. try fitting the model using original and scaled data and compare the results, but you only tried it on unscaled data\n",
    "\t  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<b>Reviewer's comment V2</b>\n",
    "\t  \n",
    "Looks like you accidentally used unscaled data both times:\n",
    "    \n",
    "```python\n",
    "study_knn.optimize(KNN_objective, n_trials=500)\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Regression (with Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `insurance_benefits` as the target, evaluate what RMSE would be for a Linear Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your own implementation of LR. For that, recall how the linear regression task's solution is formulated in terms of LA. Check RMSE for both the original data and the scaled one. Can you see any difference in RMSE between these two cases?\n",
    "\n",
    "Let's denote\n",
    "- $X$ — feature matrix, each row is a case, each column is a feature, the first column consists of unities\n",
    "- $y$ — target (a vector)\n",
    "- $\\hat{y}$ — estimated tagret (a vector)\n",
    "- $w$ — weight vector\n",
    "\n",
    "The task of linear regression in the language of matrices can be formulated as\n",
    "\n",
    "$$\n",
    "y = Xw\n",
    "$$\n",
    "\n",
    "The training objective then is to find such $w$ that it would minimize the L2-distance (MSE) between $Xw$ and $y$:\n",
    "\n",
    "$$\n",
    "\\min_w d_2(Xw, y) \\quad \\text{or} \\quad \\min_w \\text{MSE}(Xw, y)\n",
    "$$\n",
    "\n",
    "It appears that there is analytical solution for the above:\n",
    "\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "The formula above can be used to find the weights $w$ and the latter can be used to calculate predicted values\n",
    "\n",
    "$$\n",
    "\\hat{y} = X_{val}w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the whole data in the 70:30 proportion for the training/validation parts. Use the RMSE metric for the model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # adding the unities\n",
    "        X2 = np.append(np.ones([len(X), 1]), X, axis=1)\n",
    "        self.weights = np.linalg.inv(X2.T.dot(X2)).dot(X2.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # adding the unities\n",
    "        X2 = np.append(np.ones([len(X), 1]), X, axis=1)\n",
    "        y_pred = X2.dot(self.weights) \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_regressor(y_true, y_pred):\n",
    "    \n",
    "    rmse = math.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "    print(f'RMSE: {rmse:.2f}')\n",
    "    \n",
    "    r2_score = math.sqrt(sklearn.metrics.r2_score(y_true, y_pred))\n",
    "    print(f'R2: {r2_score:.2f}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['age', 'gender', 'income', 'family_members']].to_numpy()\n",
    "y = df['insurance_benefits'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "\n",
    "lr = MyLinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.weights)\n",
    "\n",
    "y_test_pred = lr.predict(X_test)\n",
    "eval_regressor(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = df_scaled[['age', 'gender', 'income', 'family_members']].to_numpy()\n",
    "y_scaled = df_scaled['insurance_benefits'].to_numpy()\n",
    "\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled, y_scaled, test_size=0.3, random_state=12345)\n",
    "\n",
    "lr = MyLinearRegression()\n",
    "\n",
    "lr.fit(X_train_scaled, y_train_scaled)\n",
    "print(lr.weights)\n",
    "\n",
    "y_test_pred_scaled = lr.predict(X_test_scaled)\n",
    "eval_regressor(y_test_scaled, y_test_pred_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see above, there is no difference between the RMSE and R2 values with the scaled and unscaled data. This shows that for linear regressions, scaling is not necessary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\t  \n",
    "Great, you correctly implemented linear regression and compared its results on scaled and unscaled data\n",
    "\t  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Obfuscating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It best to obfuscate data by multiplying the numerical features (remember, they can be seen as the matrix $X$) by an invertible matrix $P$. \n",
    "\n",
    "$$\n",
    "X' = X \\times P\n",
    "$$\n",
    "\n",
    "Try to do that and check how the features' values will look like after the transformation. By the way, the intertible property is important here so make sure that $P$ is indeed invertible.\n",
    "\n",
    "You may want to review the 'Matrices and Matrix Operations -> Matrix Mupliplication' lesson to recall the rule of matrix multiplication and its implementation with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_info_column_list = ['gender', 'age', 'income', 'family_members']\n",
    "df_pn = df[personal_info_column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_pn.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a random matrix $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=42)\n",
    "P = rng.random(size=(X.shape[1], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the matrix $P$ is invertible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if it is invertible\n",
    "P_inv = np.linalg.inv(P)\n",
    "#transforming the data\n",
    "transformed_data = X @ P\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess the customers' ages or income after the transformation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No, it is impossible to find out this information without the key matrix (P).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you recover the original data from $X'$ if you know $P$? Try to check that with calculations by moving $P$ from the right side of the formula above to the left one. The rules of matrix multiplcation are really helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X x P x P_inv = X\n",
    "recovered_data = transformed_data @ P_inv\n",
    "print(recovered_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all three cases for a few customers\n",
    "- The original data\n",
    "- The transformed one\n",
    "- The reversed (recovered) one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Data')\n",
    "print(X[:4])\n",
    "print('Transformed Data')\n",
    "print(transformed_data[:4])\n",
    "print('Recovered Data')\n",
    "print(recovered_data[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can probably see that some values are not exactly the same as they are in the original data. What might be the reason for that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This would be due to rounding errors.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\t  \n",
    "Yeah, pretty much!\n",
    "\t  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof That Data Obfuscation Can Work with LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression task has been solved with linear regression in this project. Your next task is to prove _analytically_ that the given obfuscation method won't affect linear regression in terms of predicted values i.e. their values will remain the same. Can you believe that? Well, you don't have to, you should prove it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the data is obfuscated and there is $X \\times P$ instead of just $X$ now. Consequently, there are other weights $w_P$ as\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y \\quad \\Rightarrow \\quad w_P = [(XP)^T XP]^{-1} (XP)^T y\n",
    "$$\n",
    "\n",
    "How would $w$ and $w_P$ be linked if you simplify the formula for $w_P$ above? \n",
    "\n",
    "What would be predicted values with $w_P$? \n",
    "\n",
    "What does that mean for the quality of linear regression if you measure it with RMSE?\n",
    "\n",
    "Check Appendix B Properties of Matrices in the end of the notebook. There are useful formulas in there!\n",
    "\n",
    "No code is necessary in this section, only analytical explanation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $w_p$ is equal to $P^{-1}w$ as seen below:\n",
    "$$\n",
    "\\begin{align*}\n",
    "w_P &= [(XP)^T XP]^{-1} (XP)^T y & \\text{by the solved value of regression weights}\\\\\n",
    "    &= [P^T X^T X P]^{-1} P^T X^T y & \\text{by the reversivity of the transpose of a product of matrices} \\\\\n",
    "    &= P^{-1} (X^{T} X)^{-1} (P^{T})^{-1} P^T X^T y & \\text{by the third multiplicative identity property of matrices}\\\\ \n",
    "    &= P^{-1} (X^{T} X)^{-1} X^T y & \\text{by the second multiplicative identity property of matrices}\\\\ \n",
    "    &= P^{-1} w & \\text{by the solved value of regression weights}\n",
    "\\end{align*}\n",
    "$$\n",
    "- $\\hat{y}$ with Obfuscation is $XPw_P$ and without is $Xw$, yet the proof below shows those are the exact same values.   \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y} &= XPw_P & \\text{by the solved value of target predictions}\\\\\n",
    "    &= X P P^{-1} w  & \\text{from the proof above}\\\\\n",
    "    &= X w & \\text{by the second multiplicative identity property of matrices}\\\\ \\\\ \n",
    "\\end{align*}\n",
    "$$\n",
    "- The RMSE will be the exact same as the $\\hat{y}$ values are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical proof**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<s><b>Reviewer's comment</b>\n",
    "\t  \n",
    "> $w_p$ is just $w$ multiplied by $P^{-1}$\n",
    "    \n",
    "That is correct, but there are two different ways to multiply $w$ by $P^{-1}$: $P^{-1}w$ and $w P^{-1}$. Which one is it? Also could you prove it please (don't forget properties of matrices listed at the end of the notebook)?\n",
    "    \n",
    "> The predicted values are just $X_{val}wP^{-1}$\n",
    "    \n",
    "Are you sure that it is a correct expression? What will the shape of such a product be?\n",
    "    \n",
    "> The RMSE will be the exact same as the P matrix just shifted the values and did not change the distance between them.\n",
    "    \n",
    "I don't really see how that follows from the previous lines. As I see it, the easiest way to prove that the quality won't change is to prove that predictions will be the same. As predictions equal $\\hat{y} = Xw$, the predictions of the model trained on obfuscated data will be $\\hat{y_p} = X_p w_p$, so we just need to plug in the expressions for $X_p$ and $w_p$ and compare the resulting $\\hat{y_p}$ with $\\hat{y}$\n",
    "\t  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<s><b>Reviewer's comment V2</b>\n",
    "\t  \n",
    "Ok, very good! One minor correction: when you're inversing the product $P^T X^T X P$, you should treat $X^T X$ as a single matrix, like this:\n",
    "$$[P^T X^T X P]^{-1} = P^{-1} (X^T X)^{-1} (P^T)^{-1}.$$\n",
    "    \n",
    "The reason is that $X$ (and $X^T$) is not invertible: it's not even square almost always, because the number of examples (the number of rows) is almost always much bigger than the number of features (the number of columns). And only square matrices can be invertible. This means that $X^{-1}$ is not defined.\n",
    "    \n",
    "By the way, this is basically the reason the training formula is $w = (X^T X)^{-1} X^T y$, and not just $w = X^{-1} y$ (the second formula only works if $X$ is invertible). If you ever wondered where the $(X^T X)^{-1} X^T$ comes from, here's an idea. We know that $y = Xw$, and we would like to figure out what $w$ is equal to, but we can't just multiply both sides by $X^{-1}$ to do it, as $X$ is almost always not invertible. But if we multiply both sides by $X^T$: $$X^T X w = X^T y,$$ then $X^T X$ is always square and almost always invertible (as long as there are no linearly dependent columns), so we can multiply both sides by $(X^T X)^{-1}$ to express $w$: $$w = (X^T X)^{-1} X^T y.$$\n",
    "    \n",
    "\n",
    "\t  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment V3</b>\n",
    "\t  \n",
    "Excellent, now everything is correct!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Linear Regression With Data Obfuscation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prove Linear Regression can work computationally with the chosen obfuscation transformation.\n",
    "\n",
    "Build a procedure or a class that runs Linear Regression optionally with the obfuscation. You can use either a ready implementation of Linear Regression from sciki-learn or your own.\n",
    "\n",
    "Run Linear Regression for the original data and the obfuscated one, compare the predicted values and the RMSE, $R^2$ metric values. Is there any difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Procedure**\n",
    "\n",
    "- Create a square matrix $P$ of random numbers.\n",
    "- Check that it is invertible. If not, repeat the first point until we get an invertible matrix.\n",
    "- Multiply the matrices to get $XP$\n",
    "- Use $XP$ as the new feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.weights = None\n",
    "        \n",
    "    def obfuscation(self,X):\n",
    "            n = 0\n",
    "            P = np.random.default_rng(n).random(size=(X.shape[1], X.shape[1]))\n",
    "            P_inv = None\n",
    "            while P_inv is None:\n",
    "                try: \n",
    "                    P_inv = np.linalg.inv(P)\n",
    "                except:\n",
    "                    pass\n",
    "                n = n+1\n",
    "            self.XP = X @ P\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # adding the unities\n",
    "        try:\n",
    "            X = self.XP\n",
    "        except:\n",
    "            X = X\n",
    "        X2 = np.append(np.ones([len(X), 1]), X, axis=1)\n",
    "        self.weights = np.linalg.inv(X2.T.dot(X2)).dot(X2.T).dot(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # adding the unities\n",
    "        X2 = np.append(np.ones([len(X), 1]), X, axis=1)\n",
    "        y_pred = X2.dot(self.weights) \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLinearRegression()\n",
    "model.obfuscation(X_train) \n",
    "model.fit(X_train,y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "eval_regressor(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_2 = MyLinearRegression()\n",
    "model_2.fit(X_train,y_train)\n",
    "y_test_pred_2 = model_2.predict(X_test)\n",
    "eval_regressor(y_test, y_test_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are no differences in the RMSE and R2 values when using obfuscation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\t  \n",
    "Great, you successfully validated our data obfuscation algorithm empirically.\n",
    "\t  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I helped Sure Tomorrow Insurance answer questions about how they can use Machine Learning. I started out by creating a function to find customers who are similar to a given customer. I then showed that for a given customer scaling the data has a much larger effect on the chosen similar customers than the measurement technique. \n",
    "\n",
    "Next, compared 4 random classification models to a kNN classification model for both the original and scaled data. This showed that, due to the large class imbalance, a kNN model would need some sort of resampling to be effective. \n",
    "\n",
    "Lastly, I showed that linear regressions can be transformed without any changes in the resulting RMSE and R2 values. This provides Sure Tomorrow with a model that can use obfuscated data to help protect the privacy of their clients.\n",
    "\n",
    "Overall, I recommend that Sure Tomorrow Insurance uses a linear regression model with obfuscation to predict the number of insurance benefits a new customer is likely to receive while also protecting their privacy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Reviewer's comment</b>\n",
    "\t  \n",
    "Ok, well done!\n",
    "\t  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 'x' to check. Then press Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook is open\n",
    "- [x]  Code is error free\n",
    "- [x]  The cells are arranged in order of logic and execution\n",
    "- [x]  Task 1 has been performed\n",
    "    - [x]  There is the procedure that can return k similar customers for a given one\n",
    "    - [x]  The procedure is tested for all four proposed combinations\n",
    "    - [x]  The questions re the scaling/distances are answered\n",
    "- [x]  Task 2 has been performed\n",
    "    - [x]  The random classification model is built and tested for all for probability levels\n",
    "    - [x]  The kNN classification model is built and tested for both the original data and the scaled one, the F1 metric is calculated.\n",
    "- [x]  Task 3 has been performed\n",
    "    - [x]  The linear tegression solution is implemented with matrix operations.\n",
    "    - [x]  RMSE is calculated for the implemented solution.\n",
    "- [x]  Task 4 has been performed\n",
    "    - [x]  The data is obfuscated with a random and invertible matrix P\n",
    "    - [x]  The obfuscated data is recoved, few examples are printed out\n",
    "    - [x]  The analytical proof that the transformation does not affect RMSE is provided \n",
    "    - [x]  The computational proof that the transformation does not affect RMSE is provided\n",
    "- [x]  Conclusions have been made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices \n",
    "\n",
    "## Appendix A: Writing Formulas in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write formulas in your Jupyter Notebook in a markup language provided by a high-quality publishing system called $\\LaTeX$ (pronounced \"Lah-tech\"), and they will look like formulas in textbooks.\n",
    "\n",
    "To put a formula in a text, put the dollar sign (\\\\$) before and after the formula's text e.g. $\\frac{1}{2} \\times \\frac{3}{2} = \\frac{3}{4}$ or $y = x^2, x \\ge 1$.\n",
    "\n",
    "If a formula should be in its own paragraph, put the double dollar sign (\\\\$\\\\$) before and after the formula text e.g.\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i.\n",
    "$$\n",
    "\n",
    "The markup language of [LaTeX](https://en.wikipedia.org/wiki/LaTeX) is very popular among people who use formulas in their articles, books and texts. It can be complex but its basics are easy. Check this two page [cheatsheet](http://tug.ctan.org/info/undergradmath/undergradmath.pdf) for learning how to compose the most common formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Properties of Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices have many properties in Linear Algebra. A few of them are listed here which can help with the analytical proof in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>Distributivity</td><td>$A(B+C)=AB+AC$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Non-commutativity</td><td>$AB \\neq BA$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Associative property of multiplication</td><td>$(AB)C = A(BC)$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Multiplicative identity property</td><td>$IA = AI = A$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td></td><td>$A^{-1}A = AA^{-1} = I$\n",
    "</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td></td><td>$(AB)^{-1} = B^{-1}A^{-1}$</td>\n",
    "</tr>    \n",
    "<tr>\n",
    "<td>Reversivity of the transpose of a product of matrices,</td><td>$(AB)^T = B^TA^T$</td>\n",
    "</tr>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2775,
    "start_time": "2021-11-01T18:31:42.318Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:31:47.847Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-01T18:31:50.961Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:31:53.379Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T18:31:55.230Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T18:31:58.714Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:32:16.774Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-01T18:32:22.430Z"
   },
   {
    "duration": 24,
    "start_time": "2021-11-01T18:32:30.008Z"
   },
   {
    "duration": 5108,
    "start_time": "2021-11-01T18:32:40.667Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T18:33:03.647Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T18:33:50.639Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-01T18:34:09.178Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T18:35:20.215Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T18:35:23.776Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T18:35:38.174Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:36:25.968Z"
   },
   {
    "duration": 274,
    "start_time": "2021-11-01T18:36:37.692Z"
   },
   {
    "duration": 544,
    "start_time": "2021-11-01T18:36:47.581Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T18:37:12.593Z"
   },
   {
    "duration": 2,
    "start_time": "2021-11-01T18:37:17.939Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T18:37:28.505Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:42:03.791Z"
   },
   {
    "duration": 257,
    "start_time": "2021-11-01T18:42:19.792Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:42:25.867Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-01T18:42:38.181Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:42:47.839Z"
   },
   {
    "duration": 261,
    "start_time": "2021-11-01T18:43:03.784Z"
   },
   {
    "duration": 41,
    "start_time": "2021-11-01T18:43:11.112Z"
   },
   {
    "duration": 41,
    "start_time": "2021-11-01T18:43:18.286Z"
   },
   {
    "duration": 250,
    "start_time": "2021-11-01T18:43:54.840Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T18:43:59.960Z"
   },
   {
    "duration": 600,
    "start_time": "2021-11-01T18:44:01.968Z"
   },
   {
    "duration": 254,
    "start_time": "2021-11-01T18:44:57.362Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:45:00.626Z"
   },
   {
    "duration": 965,
    "start_time": "2021-11-01T18:45:03.085Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T18:46:02.482Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:46:12.641Z"
   },
   {
    "duration": 39,
    "start_time": "2021-11-01T18:46:18.410Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:46:36.736Z"
   },
   {
    "duration": 39,
    "start_time": "2021-11-01T18:46:39.224Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:47:13.851Z"
   },
   {
    "duration": 39,
    "start_time": "2021-11-01T18:47:17.360Z"
   },
   {
    "duration": 98,
    "start_time": "2021-11-01T18:47:44.833Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:47:47.967Z"
   },
   {
    "duration": 40,
    "start_time": "2021-11-01T18:47:50.791Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:51:02.536Z"
   },
   {
    "duration": 57,
    "start_time": "2021-11-01T18:51:06.782Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:52:52.629Z"
   },
   {
    "duration": 56,
    "start_time": "2021-11-01T18:52:55.861Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:53:21.690Z"
   },
   {
    "duration": 56,
    "start_time": "2021-11-01T18:53:23.922Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:54:38.601Z"
   },
   {
    "duration": 550,
    "start_time": "2021-11-01T18:54:40.843Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:58:17.530Z"
   },
   {
    "duration": 571,
    "start_time": "2021-11-01T18:58:20.096Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T18:58:43.250Z"
   },
   {
    "duration": 565,
    "start_time": "2021-11-01T18:58:45.474Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T18:59:29.587Z"
   },
   {
    "duration": 53,
    "start_time": "2021-11-01T18:59:32.098Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:00:40.541Z"
   },
   {
    "duration": 47,
    "start_time": "2021-11-01T19:00:42.817Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:01:06.408Z"
   },
   {
    "duration": 44,
    "start_time": "2021-11-01T19:01:10.268Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:01:28.114Z"
   },
   {
    "duration": 46,
    "start_time": "2021-11-01T19:01:31.899Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:02:38.887Z"
   },
   {
    "duration": 44,
    "start_time": "2021-11-01T19:02:41.428Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:02:53.461Z"
   },
   {
    "duration": 45,
    "start_time": "2021-11-01T19:02:55.992Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T19:03:13.179Z"
   },
   {
    "duration": 45,
    "start_time": "2021-11-01T19:03:15.504Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:05:32.901Z"
   },
   {
    "duration": 563,
    "start_time": "2021-11-01T19:05:35.180Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T19:05:55.461Z"
   },
   {
    "duration": 48,
    "start_time": "2021-11-01T19:05:58.240Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T19:07:19.500Z"
   },
   {
    "duration": 3583,
    "start_time": "2021-11-01T19:07:22.117Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:07:32.270Z"
   },
   {
    "duration": 60,
    "start_time": "2021-11-01T19:07:34.200Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T19:09:33.327Z"
   },
   {
    "duration": 96,
    "start_time": "2021-11-01T19:09:48.611Z"
   },
   {
    "duration": 90,
    "start_time": "2021-11-01T19:10:18.210Z"
   },
   {
    "duration": 526,
    "start_time": "2021-11-01T19:10:56.203Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:11:16.588Z"
   },
   {
    "duration": 516,
    "start_time": "2021-11-01T19:11:19.538Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:11:36.770Z"
   },
   {
    "duration": 637,
    "start_time": "2021-11-01T19:11:45.297Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T19:12:20.923Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-01T19:12:33.162Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-01T19:12:46.753Z"
   },
   {
    "duration": 503,
    "start_time": "2021-11-01T19:13:04.606Z"
   },
   {
    "duration": 527,
    "start_time": "2021-11-01T19:14:08.457Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T19:14:48.638Z"
   },
   {
    "duration": 559,
    "start_time": "2021-11-01T19:14:51.266Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:15:07.562Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:15:31.987Z"
   },
   {
    "duration": 537,
    "start_time": "2021-11-01T19:15:33.697Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:15:49.246Z"
   },
   {
    "duration": 710,
    "start_time": "2021-11-01T19:15:52.671Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:16:04.013Z"
   },
   {
    "duration": 659,
    "start_time": "2021-11-01T19:16:06.569Z"
   },
   {
    "duration": 2703,
    "start_time": "2021-11-01T19:16:46.929Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T19:16:49.635Z"
   },
   {
    "duration": 24,
    "start_time": "2021-11-01T19:16:49.640Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T19:16:49.666Z"
   },
   {
    "duration": 16,
    "start_time": "2021-11-01T19:16:49.674Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-01T19:16:49.691Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-01T19:16:49.701Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-01T19:16:49.715Z"
   },
   {
    "duration": 29,
    "start_time": "2021-11-01T19:16:49.726Z"
   },
   {
    "duration": 5332,
    "start_time": "2021-11-01T19:16:49.757Z"
   },
   {
    "duration": 2,
    "start_time": "2021-11-01T19:16:55.091Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:16:58.629Z"
   },
   {
    "duration": 546,
    "start_time": "2021-11-01T19:17:02.151Z"
   },
   {
    "duration": 294,
    "start_time": "2021-11-01T19:18:16.453Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T19:18:31.700Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:18:38.370Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-01T19:18:46.036Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-01T19:20:43.165Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T19:20:45.338Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-01T19:20:54.238Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-01T19:21:23.068Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-01T19:21:34.629Z"
   },
   {
    "duration": 8194,
    "start_time": "2021-11-01T19:21:54.175Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-01T19:22:06.860Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-01T19:22:23.294Z"
   },
   {
    "duration": 38,
    "start_time": "2021-11-01T19:22:27.468Z"
   },
   {
    "duration": 86,
    "start_time": "2021-11-01T19:22:51.898Z"
   },
   {
    "duration": 82,
    "start_time": "2021-11-01T19:22:59.717Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-01T19:23:04.699Z"
   },
   {
    "duration": 16,
    "start_time": "2021-11-01T19:23:10.354Z"
   },
   {
    "duration": 6736,
    "start_time": "2021-11-01T19:26:33.145Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-01T19:26:51.310Z"
   },
   {
    "duration": 33,
    "start_time": "2021-11-01T19:28:14.533Z"
   },
   {
    "duration": 294,
    "start_time": "2021-11-01T19:41:00.024Z"
   },
   {
    "duration": 433,
    "start_time": "2021-11-01T19:42:05.622Z"
   },
   {
    "duration": 67105,
    "start_time": "2021-11-01T19:42:28.165Z"
   },
   {
    "duration": -894,
    "start_time": "2021-11-01T19:43:36.167Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T19:43:46.607Z"
   },
   {
    "duration": 132082,
    "start_time": "2021-11-01T19:43:51.497Z"
   },
   {
    "duration": 132995,
    "start_time": "2021-11-01T19:47:10.363Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T19:49:45.132Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T19:49:51.523Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T19:50:09.095Z"
   },
   {
    "duration": 110931,
    "start_time": "2021-11-01T19:52:34.501Z"
   },
   {
    "duration": 6255,
    "start_time": "2021-11-01T19:54:31.100Z"
   },
   {
    "duration": 25941,
    "start_time": "2021-11-01T19:54:44.789Z"
   },
   {
    "duration": 25504,
    "start_time": "2021-11-01T19:56:08.491Z"
   },
   {
    "duration": 8080,
    "start_time": "2021-11-01T19:56:48.047Z"
   },
   {
    "duration": 30981,
    "start_time": "2021-11-01T19:57:02.982Z"
   },
   {
    "duration": 38730,
    "start_time": "2021-11-01T19:57:36.535Z"
   },
   {
    "duration": 55936,
    "start_time": "2021-11-01T19:58:17.609Z"
   },
   {
    "duration": 271,
    "start_time": "2021-11-01T19:59:18.867Z"
   },
   {
    "duration": 1458,
    "start_time": "2021-11-01T19:59:23.543Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-01T20:00:14.834Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-01T20:00:22.610Z"
   },
   {
    "duration": 13,
    "start_time": "2021-11-01T20:00:34.444Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-01T20:00:46.754Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-01T20:01:05.858Z"
   },
   {
    "duration": 19,
    "start_time": "2021-11-01T20:01:55.361Z"
   },
   {
    "duration": 39,
    "start_time": "2021-11-01T20:02:30.409Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-01T20:02:53.540Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-01T20:03:00.500Z"
   },
   {
    "duration": 268,
    "start_time": "2021-11-01T20:03:48.914Z"
   },
   {
    "duration": 283,
    "start_time": "2021-11-01T20:04:35.123Z"
   },
   {
    "duration": 277,
    "start_time": "2021-11-01T20:05:36.569Z"
   },
   {
    "duration": 27569,
    "start_time": "2021-11-01T20:05:45.314Z"
   },
   {
    "duration": 290,
    "start_time": "2021-11-01T20:06:58.968Z"
   },
   {
    "duration": 4549,
    "start_time": "2021-11-01T20:12:40.628Z"
   },
   {
    "duration": 261,
    "start_time": "2021-11-01T20:12:49.561Z"
   },
   {
    "duration": 263,
    "start_time": "2021-11-01T20:13:29.530Z"
   },
   {
    "duration": 269,
    "start_time": "2021-11-01T20:13:35.955Z"
   },
   {
    "duration": 276,
    "start_time": "2021-11-01T20:14:00.536Z"
   },
   {
    "duration": 132685,
    "start_time": "2021-11-01T20:14:11.381Z"
   },
   {
    "duration": 132476,
    "start_time": "2021-11-01T20:20:19.148Z"
   },
   {
    "duration": 14,
    "start_time": "2021-11-01T20:30:00.332Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T20:30:11.340Z"
   },
   {
    "duration": 28,
    "start_time": "2021-11-01T20:30:35.638Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T20:31:05.747Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-01T20:31:09.686Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T20:31:47.396Z"
   },
   {
    "duration": 270,
    "start_time": "2021-11-01T20:35:21.729Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T20:35:41.450Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-01T20:35:47.184Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-01T20:35:53.058Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-01T20:37:09.320Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T20:38:27.111Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T20:44:28.438Z"
   },
   {
    "duration": 548,
    "start_time": "2021-11-01T20:44:31.369Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T20:46:40.440Z"
   },
   {
    "duration": 37,
    "start_time": "2021-11-01T20:48:07.579Z"
   },
   {
    "duration": 50,
    "start_time": "2021-11-01T20:50:36.880Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T21:02:10.323Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T21:02:18.402Z"
   },
   {
    "duration": 548,
    "start_time": "2021-11-01T21:03:20.268Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T21:04:32.688Z"
   },
   {
    "duration": 579,
    "start_time": "2021-11-01T21:04:36.019Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T21:04:58.172Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-01T21:05:03.964Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T21:06:42.349Z"
   },
   {
    "duration": 1330,
    "start_time": "2021-11-01T21:06:45.141Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T21:07:14.747Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-01T21:07:16.713Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-01T21:08:25.517Z"
   },
   {
    "duration": 253,
    "start_time": "2021-11-01T21:12:21.450Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T21:12:25.457Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T21:12:27.958Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T21:12:30.463Z"
   },
   {
    "duration": 762,
    "start_time": "2021-11-01T21:12:32.873Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T21:12:38.161Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T21:14:57.636Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T21:15:05.262Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T21:16:21.292Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T21:16:24.810Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T21:16:32.933Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T21:16:40.803Z"
   },
   {
    "duration": 276,
    "start_time": "2021-11-01T21:18:21.815Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T21:18:48.764Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T21:19:04.295Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T21:23:00.036Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:14:42.499Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T22:15:11.803Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:15:41.662Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-01T22:17:12.042Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T22:17:22.434Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:17:46.029Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T22:17:48.126Z"
   },
   {
    "duration": 350,
    "start_time": "2021-11-01T22:18:09.082Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:18:15.763Z"
   },
   {
    "duration": 258,
    "start_time": "2021-11-01T22:18:18.105Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:19:42.869Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T22:19:46.947Z"
   },
   {
    "duration": 268,
    "start_time": "2021-11-01T22:21:22.590Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T22:21:30.719Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-01T22:21:35.348Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-01T22:22:25.205Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:22:44.099Z"
   },
   {
    "duration": 570,
    "start_time": "2021-11-01T22:23:57.709Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:24:08.997Z"
   },
   {
    "duration": 587,
    "start_time": "2021-11-01T22:25:32.380Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T22:26:31.841Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:26:33.944Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T22:26:35.435Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:27:13.423Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:27:58.189Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:30:07.716Z"
   },
   {
    "duration": 1221,
    "start_time": "2021-11-01T22:30:11.511Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:31:45.082Z"
   },
   {
    "duration": 707,
    "start_time": "2021-11-01T22:31:48.331Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-01T22:32:17.016Z"
   },
   {
    "duration": 2,
    "start_time": "2021-11-01T22:32:19.051Z"
   },
   {
    "duration": 2,
    "start_time": "2021-11-01T22:32:23.680Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T22:32:37.709Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:33:00.658Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T22:33:03.199Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:35:38.363Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T22:35:40.490Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-01T22:35:54.781Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-01T22:35:56.963Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:38:28.628Z"
   },
   {
    "duration": 568,
    "start_time": "2021-11-01T22:38:30.884Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:39:00.446Z"
   },
   {
    "duration": 516,
    "start_time": "2021-11-01T22:39:02.468Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-01T22:39:23.560Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-01T22:39:25.378Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-01T22:39:55.998Z"
   },
   {
    "duration": 1563,
    "start_time": "2021-11-02T02:00:34.401Z"
   },
   {
    "duration": 339,
    "start_time": "2021-11-02T02:00:38.612Z"
   },
   {
    "duration": 18202,
    "start_time": "2021-11-02T02:00:45.591Z"
   },
   {
    "duration": 676,
    "start_time": "2021-11-02T02:01:03.795Z"
   },
   {
    "duration": 564,
    "start_time": "2021-11-02T02:05:32.164Z"
   },
   {
    "duration": 5205,
    "start_time": "2021-11-02T02:05:46.467Z"
   },
   {
    "duration": 1718,
    "start_time": "2021-11-02T02:05:51.674Z"
   },
   {
    "duration": 17,
    "start_time": "2021-11-02T02:05:53.394Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-02T02:05:53.413Z"
   },
   {
    "duration": 14,
    "start_time": "2021-11-02T02:05:53.418Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-02T02:05:53.433Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-02T02:05:53.441Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-02T02:05:53.446Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-02T02:05:53.455Z"
   },
   {
    "duration": 4772,
    "start_time": "2021-11-02T02:05:53.477Z"
   },
   {
    "duration": 2,
    "start_time": "2021-11-02T02:05:58.251Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-02T02:05:58.255Z"
   },
   {
    "duration": 21,
    "start_time": "2021-11-02T02:05:58.263Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-02T02:05:58.285Z"
   },
   {
    "duration": 123444,
    "start_time": "2021-11-02T02:05:58.297Z"
   },
   {
    "duration": -28,
    "start_time": "2021-11-02T02:08:01.771Z"
   },
   {
    "duration": -29,
    "start_time": "2021-11-02T02:08:01.773Z"
   },
   {
    "duration": -29,
    "start_time": "2021-11-02T02:08:01.774Z"
   },
   {
    "duration": -213,
    "start_time": "2021-11-02T02:08:01.959Z"
   },
   {
    "duration": -216,
    "start_time": "2021-11-02T02:08:01.963Z"
   },
   {
    "duration": -217,
    "start_time": "2021-11-02T02:08:01.965Z"
   },
   {
    "duration": -217,
    "start_time": "2021-11-02T02:08:01.966Z"
   },
   {
    "duration": -217,
    "start_time": "2021-11-02T02:08:01.968Z"
   },
   {
    "duration": -217,
    "start_time": "2021-11-02T02:08:01.969Z"
   },
   {
    "duration": -220,
    "start_time": "2021-11-02T02:08:01.973Z"
   },
   {
    "duration": -220,
    "start_time": "2021-11-02T02:08:01.974Z"
   },
   {
    "duration": -221,
    "start_time": "2021-11-02T02:08:01.976Z"
   },
   {
    "duration": -221,
    "start_time": "2021-11-02T02:08:01.977Z"
   },
   {
    "duration": -409,
    "start_time": "2021-11-02T02:08:02.166Z"
   },
   {
    "duration": -415,
    "start_time": "2021-11-02T02:08:02.173Z"
   },
   {
    "duration": -415,
    "start_time": "2021-11-02T02:08:02.175Z"
   },
   {
    "duration": -415,
    "start_time": "2021-11-02T02:08:02.176Z"
   },
   {
    "duration": -416,
    "start_time": "2021-11-02T02:08:02.178Z"
   },
   {
    "duration": -417,
    "start_time": "2021-11-02T02:08:02.180Z"
   },
   {
    "duration": -417,
    "start_time": "2021-11-02T02:08:02.181Z"
   },
   {
    "duration": 123433,
    "start_time": "2021-11-02T02:08:32.874Z"
   },
   {
    "duration": 14,
    "start_time": "2021-11-02T02:10:46.230Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-02T02:10:48.073Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-02T02:10:50.895Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-02T02:10:52.890Z"
   },
   {
    "duration": 45,
    "start_time": "2021-11-02T02:10:54.352Z"
   },
   {
    "duration": 50,
    "start_time": "2021-11-02T02:10:56.733Z"
   },
   {
    "duration": 22079,
    "start_time": "2021-11-02T02:10:59.219Z"
   },
   {
    "duration": 21252,
    "start_time": "2021-11-02T02:11:34.031Z"
   },
   {
    "duration": 21860,
    "start_time": "2021-11-02T02:12:37.085Z"
   },
   {
    "duration": 21675,
    "start_time": "2021-11-02T02:13:36.170Z"
   },
   {
    "duration": 22339,
    "start_time": "2021-11-02T02:16:24.425Z"
   },
   {
    "duration": 41905,
    "start_time": "2021-11-02T02:17:17.352Z"
   },
   {
    "duration": 48,
    "start_time": "2021-11-02T02:18:02.037Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-02T02:19:31.068Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-02T02:19:33.400Z"
   },
   {
    "duration": 10,
    "start_time": "2021-11-02T02:19:35.710Z"
   },
   {
    "duration": 9,
    "start_time": "2021-11-02T02:19:40.354Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-02T02:29:14.545Z"
   },
   {
    "duration": 3,
    "start_time": "2021-11-02T02:29:16.359Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-02T02:29:19.011Z"
   },
   {
    "duration": 93,
    "start_time": "2021-11-02T02:29:20.532Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-02T02:29:22.829Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-02T02:29:25.187Z"
   },
   {
    "duration": 5,
    "start_time": "2021-11-02T02:29:27.450Z"
   },
   {
    "duration": 31471,
    "start_time": "2021-11-02T08:29:16.917Z"
   },
   {
    "duration": 2834,
    "start_time": "2021-11-02T08:29:48.392Z"
   },
   {
    "duration": 20,
    "start_time": "2021-11-02T08:29:51.231Z"
   },
   {
    "duration": 6,
    "start_time": "2021-11-02T08:29:51.255Z"
   },
   {
    "duration": 28,
    "start_time": "2021-11-02T08:29:51.265Z"
   },
   {
    "duration": 11,
    "start_time": "2021-11-02T08:29:51.296Z"
   },
   {
    "duration": 16,
    "start_time": "2021-11-02T08:29:51.309Z"
   },
   {
    "duration": 26,
    "start_time": "2021-11-02T08:29:51.327Z"
   },
   {
    "duration": 43,
    "start_time": "2021-11-02T08:29:51.357Z"
   },
   {
    "duration": 8977,
    "start_time": "2021-11-02T08:29:51.403Z"
   },
   {
    "duration": 4,
    "start_time": "2021-11-02T08:30:00.383Z"
   },
   {
    "duration": 15,
    "start_time": "2021-11-02T08:30:00.390Z"
   },
   {
    "duration": 35,
    "start_time": "2021-11-02T08:30:00.408Z"
   },
   {
    "duration": 34,
    "start_time": "2021-11-02T08:30:00.445Z"
   },
   {
    "duration": 121907,
    "start_time": "2021-11-02T08:30:00.481Z"
   },
   {
    "duration": -259,
    "start_time": "2021-11-02T08:32:02.651Z"
   },
   {
    "duration": -263,
    "start_time": "2021-11-02T08:32:02.656Z"
   },
   {
    "duration": -424,
    "start_time": "2021-11-02T08:32:02.818Z"
   },
   {
    "duration": -427,
    "start_time": "2021-11-02T08:32:02.823Z"
   },
   {
    "duration": -430,
    "start_time": "2021-11-02T08:32:02.827Z"
   },
   {
    "duration": -431,
    "start_time": "2021-11-02T08:32:02.830Z"
   },
   {
    "duration": -433,
    "start_time": "2021-11-02T08:32:02.833Z"
   },
   {
    "duration": -434,
    "start_time": "2021-11-02T08:32:02.836Z"
   },
   {
    "duration": -437,
    "start_time": "2021-11-02T08:32:02.840Z"
   },
   {
    "duration": -440,
    "start_time": "2021-11-02T08:32:02.844Z"
   },
   {
    "duration": -442,
    "start_time": "2021-11-02T08:32:02.848Z"
   },
   {
    "duration": -445,
    "start_time": "2021-11-02T08:32:02.852Z"
   },
   {
    "duration": -447,
    "start_time": "2021-11-02T08:32:02.856Z"
   },
   {
    "duration": -449,
    "start_time": "2021-11-02T08:32:02.859Z"
   },
   {
    "duration": -452,
    "start_time": "2021-11-02T08:32:02.864Z"
   },
   {
    "duration": -454,
    "start_time": "2021-11-02T08:32:02.867Z"
   },
   {
    "duration": -456,
    "start_time": "2021-11-02T08:32:02.870Z"
   },
   {
    "duration": -457,
    "start_time": "2021-11-02T08:32:02.873Z"
   },
   {
    "duration": -596,
    "start_time": "2021-11-02T08:32:03.013Z"
   },
   {
    "duration": -598,
    "start_time": "2021-11-02T08:32:03.017Z"
   },
   {
    "duration": 18,
    "start_time": "2021-11-02T08:32:13.613Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-02T08:32:13.634Z"
   },
   {
    "duration": 8,
    "start_time": "2021-11-02T08:32:13.644Z"
   },
   {
    "duration": 7,
    "start_time": "2021-11-02T08:32:13.655Z"
   },
   {
    "duration": 59,
    "start_time": "2021-11-02T08:32:13.665Z"
   },
   {
    "duration": 37338,
    "start_time": "2021-11-02T08:32:13.727Z"
   },
   {
    "duration": -232,
    "start_time": "2021-11-02T08:32:51.301Z"
   },
   {
    "duration": -237,
    "start_time": "2021-11-02T08:32:51.307Z"
   },
   {
    "duration": -240,
    "start_time": "2021-11-02T08:32:51.312Z"
   },
   {
    "duration": -244,
    "start_time": "2021-11-02T08:32:51.317Z"
   },
   {
    "duration": -248,
    "start_time": "2021-11-02T08:32:51.323Z"
   },
   {
    "duration": -252,
    "start_time": "2021-11-02T08:32:51.328Z"
   },
   {
    "duration": -257,
    "start_time": "2021-11-02T08:32:51.334Z"
   },
   {
    "duration": -261,
    "start_time": "2021-11-02T08:32:51.340Z"
   },
   {
    "duration": -265,
    "start_time": "2021-11-02T08:32:51.345Z"
   },
   {
    "duration": -269,
    "start_time": "2021-11-02T08:32:51.351Z"
   },
   {
    "duration": -273,
    "start_time": "2021-11-02T08:32:51.356Z"
   },
   {
    "duration": -409,
    "start_time": "2021-11-02T08:32:51.494Z"
   },
   {
    "duration": -413,
    "start_time": "2021-11-02T08:32:51.499Z"
   },
   {
    "duration": -416,
    "start_time": "2021-11-02T08:32:51.504Z"
   },
   {
    "duration": 12,
    "start_time": "2021-11-02T08:35:32.861Z"
   },
   {
    "duration": 38385,
    "start_time": "2021-11-02T08:35:45.237Z"
   },
   {
    "duration": 15422,
    "start_time": "2021-11-02T08:36:58.622Z"
   },
   {
    "duration": 85,
    "start_time": "2021-11-02T08:37:54.849Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
